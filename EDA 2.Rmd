---
title: "EDA"
author: "Tyler Swanson"
date: "2024-09-28"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Packages & Import Data

```{r}
# Load necessary packages
if (!requireNamespace("dplyr", quietly = TRUE)) {
  install.packages("dplyr")
}
library(dplyr)

if (!requireNamespace("ggplot2", quietly = TRUE)) {
  install.packages("ggplot2")
}
library(ggplot2)

if (!requireNamespace("skimr", quietly = TRUE)) {
  install.packages("skimr")
}
library(skimr)

if (!requireNamespace("janitor", quietly = TRUE)) {
  install.packages("janitor")
}
library(janitor)

# Load data
application_train <- read.csv("application_train.csv", stringsAsFactors = FALSE)
application_test <- read.csv("application_test.csv", stringsAsFactors = FALSE)
bureau <- read.csv("bureau.csv", stringsAsFactors = FALSE)
previous_application <- read.csv("previous_application.csv", stringsAsFactors = FALSE)
HomeCredit_data_dictionary <- read.csv("HomeCredit_columns_description.csv", stringsAsFactors = FALSE)
```

## Exploratory Data Analysis (EDA)

### Exploring the Target Variable

```{r}
# Analyze the distribution of the target variable
target_distribution <- application_train %>%
  group_by(TARGET) %>%
  summarise(count = n()) %>%
  mutate(percentage = count / sum(count) * 100)

print(target_distribution)

# Visualize the target distribution
ggplot(target_distribution, aes(x = factor(TARGET), y = percentage, fill = factor(TARGET))) +
  geom_bar(stat = "identity") +
  labs(title = "Target Variable Distribution", x = "Target", y = "Percentage") +
  theme_minimal()
```

### Exploring the Relationship Between Target and Predictors

```{r}
# Identify numeric and categorical variables
numeric_vars <- application_train %>% select(where(is.numeric)) %>% select(-TARGET)
categorical_vars <- application_train %>% select(where(is.character))

# Correlation with target
correlation_with_target <- sapply(numeric_vars, function(x) cor(application_train$TARGET, x, use = "complete.obs"))
correlation_df <- data.frame(variable = names(correlation_with_target), correlation = correlation_with_target)

# Top 5 predictors with strongest correlation
top_predictors <- correlation_df %>% arrange(desc(abs(correlation))) %>% head(5)
print(top_predictors)

# Visualize correlations
ggplot(top_predictors, aes(x = reorder(variable, abs(correlation)), y = correlation)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(title = "Top 5 Predictors with Strongest Correlation to Target",
       x = "Predictor Variables",
       y = "Correlation with Target") +
  theme_minimal()
```

### Categorical Variable Analysis

```{r}
# Relationship between NAME_INCOME_TYPE and TARGET
NAME_INCOME_TYPE_Data <- table(application_train$NAME_INCOME_TYPE, application_train$TARGET)
prop_table <- as.data.frame(prop.table(NAME_INCOME_TYPE_Data, margin = 1))
colnames(prop_table) <- c("NAME_INCOME_TYPE", "TARGET", "Proportion")

# Visualize relationship
ggplot(prop_table, aes(x = NAME_INCOME_TYPE, y = Proportion, fill = as.factor(TARGET))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Relationship between NAME_INCOME_TYPE and Target",
       x = "Income Type", y = "Proportion", fill = "Target") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme_minimal()
```

### Skimr Summary

```{r}
# Use skimr for detailed summary
skim(application_train)
```

## Data Cleaning

### Missing Data Analysis

```{r}
# Summarize missing data
missing_data <- application_train %>%
  summarise_all(~sum(is.na(.))) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "missing_count") %>%
  mutate(missing_percentage = (missing_count / nrow(application_train)) * 100) %>%
  arrange(desc(missing_percentage))

head(missing_data, 10)

# Remove columns with >80% missing values
columns_to_remove <- missing_data %>% filter(missing_percentage > 80) %>% pull(variable)
application_train <- application_train %>% select(-all_of(columns_to_remove))
```

### Impute Missing Values

```{r}
# Impute numeric variables with median
for (col in names(application_train %>% select(where(is.numeric)))) {
  application_train[[col]][is.na(application_train[[col]])] <- median(application_train[[col]], na.rm = TRUE)
}

# Impute categorical variables with mode
for (col in names(application_train %>% select(where(is.character)))) {
  mode_value <- names(sort(table(application_train[[col]]), decreasing = TRUE))[1]
  application_train[[col]][is.na(application_train[[col]])] <- mode_value
}
```


```{r}
#Aggregate bureau.csv and SK_ID_CURR
bureau_aggregated <- bureau %>%
  group_by(SK_ID_CURR) %>%
  summarize(
    bureau_count = n(), 
    avg_credit_active = mean(CREDIT_ACTIVE == "Active", na.rm = TRUE), 
    total_credit_amt = sum(AMT_CREDIT_SUM, na.rm = TRUE), 
    avg_days_credit = mean(DAYS_CREDIT, na.rm = TRUE) 
  )

# Aggregate previous_application.csv and SK_ID_CURR
previous_application_aggregated <- previous_application %>%
  group_by(SK_ID_CURR) %>%
  summarize(
    prev_app_count = n(), # Number of previous applications per applicant
    avg_credit_approved = mean(NAME_CONTRACT_STATUS == "Approved", na.rm = TRUE), # Proportion of approved applications
    max_amt_credit = max(AMT_CREDIT, na.rm = TRUE), # Maximum credit amount in previous applications
    avg_amt_credit = mean(AMT_CREDIT, na.rm = TRUE) # Average credit amount
  )

# Join bureau_aggregated and previous_application data with application_train
application_train_combined <- application_train %>%
  left_join(bureau_aggregated, by = "SK_ID_CURR") %>%
  left_join(previous_application_aggregated, by = "SK_ID_CURR")

# Join application_test.csv and previous_application with application_train
application_test_combined <- application_test %>%
  left_join(bureau_aggregated, by = "SK_ID_CURR") %>%
  left_join(previous_application_aggregated, by = "SK_ID_CURR")

# Verify TARGET column is in combined dataset
application_train_combined$TARGET <- application_train$TARGET

# Target variable and the new columns correlations
correlations_new_columns <- sapply(application_train_combined %>% 
                                       select(bureau_count, avg_credit_active, total_credit_amt, avg_days_credit, 
                                              prev_app_count, avg_credit_approved, max_amt_credit, avg_amt_credit),
                                     function(x) cor(application_train_combined$TARGET, x, use = "complete.obs"))

# Convert correlations to a data frame 
correlation_data_frame_added <- data.frame(
  variable = names(correlations_new_columns),
  correlation = correlations_new_columns
) %>%
  arrange(desc(abs(correlation)))

# Display top variables correlating with target
print("Target variable correlation with added variables:")

print(correlation_data_frame_added)

# Visualize the relationships
for (variable in correlation_data_frame_added$variable) {
  print(
    ggplot(application_train_combined, aes_string(x = variable, fill = "factor(TARGET)")) +
      geom_density(alpha = 0.5) +
      labs(title = paste("Distribution of", variable, "by Target"),
           x = variable, fill = "Target") +
      theme_minimal() +
      theme(legend.position = "bottom") +
      scale_fill_manual(values = c("red", "blue"))
  )
}
```



## Results
Through the EDA, key predictors were identified on the uncleaned data set. These key predictors included EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3 due to their correlation with the target variable.However, Data issues including missing values and potential outliers were discovered which requited some major data cleaning in order to prepare the data for modeling. Variable relationships with the target variable were explored, but additional analysis is needed to determine any major significance between variables. This EDA highlighted the need to handle missing data carefully and consider creating new features to make the most of the data analysis.

### Business Problem
People often face difficulties obtaining loans due to a lack of credit history. Home Credit Group seeks to improve its prediction model for loan repayment ability to provide ethical lending opportunities.

### Benefits of Solution
- Increase loan approvals for eligible clients.
- Decrease loan default rates.
- Offer clients better loan terms for financial success.

### Success Metrics
1. Higher loan approval rates for eligible clients.
2. Lower default rates.
3. Improved client financial success.

### Analytics Approach
- Supervised machine learning for classification (repayment ability: yes/no).

### Guiding Questions
1. Distribution of target variable.
2. Relationship between target and predictors.
3. Missing data patterns and handling.
4. Feature importance for prediction.

# Create a New Repository on the Command Line
echo "# EDA-2" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/tyler-swanson/EDA-2.git
git push -u origin main

# Puxh an Existing Repository from the Command Line
git remote add origin https://github.com/tyler-swanson/EDA-2.git
git branch -M main
git push -u origin main